# 部署
部署，调用。

## Hugging Face
可以将开源模型跑在提供的 AWS 服务器上。

支持 API 和 Gradio 里用。

## [Xinference](https://github.com/xorbitsai/inference)
内置支持主流的模型，也支持自定义模型。可以跑在本地。

使用模型的方式可以通过 API（兼容 OpenAI），代码调用，也能集成在 LangChain 和 LlamaIndex。

## [OpenLLM](https://github.com/bentoml/OpenLLM)
跑 OpenLLM 上的很多模型，都需要 Nvidia 的 GPU。因此 Mac 上跑不了。。。

跑不需要 GPU 的，对话的过程中会报错。。。

## [魔搭](https://modelscope.cn/home)
阿里的。

每个云厂商，基本都有类似的问题。